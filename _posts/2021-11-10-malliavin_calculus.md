---
layout: distill
title: Mallivian Calculus
description: (in progress) Some background of Mallivian Calculus 
date: 2021-11-10
authors:
  - name: Anh Tong
    url: "https://anh-tong.github.io/"
    affiliations:
      name: KAIST
output: 
  distill::distill_article:
    toc: true
    number_sections: true
    toc_depth: 4
    code_folding: true
  toc_float: 
    collapsed: false
    smooth_scroll: true
---

<d-toc>Table of contents</d-toc>



## Introduction

This is a long post distilling some concepts of Malliavin Calculus and based on the <a href="http://hairer.org/notes/Malliavin.pdf">lecture note</a> of <a href="https://en.wikipedia.org/wiki/Martin_Hairer"> Martin Hairer</a>. 

**Motivation** Malliavin calculus is a modern tool tackling with differentiating random variable defined on a Gaussian probability space w.r.t. the underlying noise.

<p>At the moment, I feel like White Noise Theory and Malliavian calculus share some similarity. They surely complement each other but I do not comprehend the difference between them, for example, what one can do but other cannot.
I also plan to get to know more some background of <a href="http://www.hairer.org/notes/RoughPaths.pdf">rough path</a> but this will be in another post. </p> 
<!-- <aside>Some text in an aside, margin notes, etc...</aside> -->

Stochastic analysis centers around stochastic differential equations, i.e.,

$$
dX_t = V_0(X_t)dt + \sum_{i=1}^m V_i(X_t) \circ dW_i(t)
$$

where $\circ dW_t$ denotes Straonovich integration. In this equation, Hairer considers multiple noise parts. 

## White noise and Wiener chaos

This section concerns the definition of white noise under functional representation. Wiener chaos gives the decomposition form of white noise in which we will find somewhat similar to Fourier analysis or <a href="Sobolev space">Sobolev space</a>. 

Now, let's talk about spaces we will work on

- $H = L^2(\mathbb{R}_+, \mathbb{R}^m)$: a real and separatable Hilbert space
- $L^2(\Omega, \mathbb{P})$: for some probability space $(\Omega, \mathbb{P})$

White noise is linear isometry<d-fn>linear map preserving distance</d-fn>s $W: H \to L^2(\Omega, \mathbb{P})$ such that the ouput $W(h)$ is a real-valued Gaussian variable or

$$\mathbb{E}[W(h)], \qquad \mathbb{E}[W(h)W(g)] = \langle h, g \rangle_H.$$

The above is just the definition. How to establish such map will be shown next.

**Orthonormal basis** Here, we define 
- a sequence of i.i.d. normal random variable $\{\xi_n\}_{n\geq 0}$
- an orthonormal basis $\{e_n\}_{n \geq 0}$ of $H$. 

When representing $h = \sum_n h_n e_n \in H$, we construct $W(h)=\sum_n h_n \xi_n$. The normal random variable $\xi_n$ is now can rewrite in the functional form form $\xi_n = W(e_n)$.

In the lecture, $m$-dimensional Wiener process is defined by using funtion $\mathbf{1}_{[0,t)}^{(i)}$

$$
(\mathbf{1}_{[0,t)}^{(i)})_j(s) = \begin{cases}
   1 &\text{if } s \in [0, t) \text{ and } j=i \\
   0 &\text{otherwise } 
\end{cases}
$$

Note that 1-dimensional case is easy to show, but for now, still follow the setup in the lecture. 

The $i$-th dimension of Wiener process is defined as $W_i(t) = W(\mathbf{1}_{[0,t)}^{(i)})$ where we can check the covariance 

$$
\mathbb{E}[W_i(t)W_j(s)] = \langle \mathbf{1}_{[0,t)}^{(i)}, \mathbf{1}_{[0,s)}^{(j)} \rangle = \delta_{ij}(t \wedge s).
$$

For arbitrary $h$, $W(h)$ can represent as (Wiener-Ito integral)

$$
W(h) = \sum_{i=1}^m \int_0^\infty h_i(s) dW_i(s)
$$

Note that we may need to clearly differentiate between $W(h)$ and $W_i(s)$. 

**Basis with Hermite polynomials**

There are several formulation of Hermite polynomial

- Recursive $H^\prime_n(x) = n H_{n-1}(x)$
- A different recursive $H_{n + 1}(x) = xH_n(x) - H^\prime_n(x)$, 
- Explicit representation $H_n(x) = \exp\left(-\frac{D^2}{2}\right) x^n$, where $D$ is the differentiation w.r.t. to $x$, $\exp(\cdot)$ is defined in the Taylor expansion sense. 

Some properties of Hermite polynomials:

- $\mathbb{E}[H_n(X)] = 0$: $H_n(X)$ has zero-mean when $X \sim \mathcal{N}(0,1)$
- Identity 

$$
\begin{aligned}
\int H_n(x) H_m(x) e^{-x^2/2} dx  = & 
\frac{1}{n + 1} \int H'_{n + 1}(x) H_m(x) e^{-x^2/2} dx \\
 = & \frac{1}{n + 1} \int H_{n + 1}(x) (xH_m(x) - H'm(x)) e^{-x^2/2} dx \\
 = & \frac{1}{n + 1} \int H_{n + 1}(x) H_{m + 1}(x) e^{-x^2/2} dx
\end{aligned}
$$

This recursive leads to $\mathbb{E}[H_n(X)H_m(X)] = n! \delta_{n.m}$

Let's define linear subspaces of $L^2(\Omega, \mathbb{P})$ as

$$
\mathcal{H} = \{H_n(W(h)), h \in H, \lvert\lvert h \rvert\rvert_H = 1\}
$$

We have the following decomposition
<br><br><strong>Theorem</strong> <i> Let $\mathcal{F}$ be the $\sigma$-algebra generated by $W$. Then,</i>

$$
L^2(\Omega, \mathcal{F}, \mathbb{P}) = \bigoplus_{n=0}^\infty \mathcal{H}_n
$$

<br><br>

**Proof** Let $X \in L^2(\Omega, \mathcal{F}, \mathbb{P})$ be orthogonal to $\mathcal{H}_n$ *for all* $n$. 

$$\mathbb{E}[XH_n(W(h))] = 0, \forall n \quad \Rightarrow \quad \mathbb{E}[X\exp(W(h))] = 0$$

We need to show that $X=0$. Splitting $X = X^+ - X^-$, and define the following measures

$$
\nu^{+,-} = \mathbb{E}[X^{+,-} \mathbf{1}_B(W(h_1), \dots, W(h_m))], \quad B \in \mathcal{B}(\mathbb{R}^m)
$$

Applying Laplace transform for $\nu$, we deduce

$$
\varphi_{\nu^{+,-}}(\lambda) = \int \exp(\lambda \cdot x) \nu^{+,-}(dx) = \mathbb{E}[X^{+,-}\exp(\sum_i \lambda_i W(h_i))] = 0
$$

As the Laplace is zero, then the measure is zero. Thus, $\mathbb{E}[X\mathbb{1}_F] = 0, \forall F \in \mathcal{F}$. Therefore, $X=0$ and we can conclude the proof.

### Multiple stochastic integrals

 This section defines multiple Wiener-Ito integral w.r.t. Brownian motion. With this definition we can lead to a similar decomposition like the representation of Hermite polynomials presented above. 

 Consider

- measure space $(T, \mathcal{B}, \mu)$ 
- one-dimensional Brownian motion $B(t), t \in T = [a, b]$
- Functional space $H=L^2([a, b], \mathbb{R})$
- Functional representation $W(h) = \int_a^b h(s) dB(s)$

Like traditional stochastic calculus, this tries to set up a corner stone with elementary process

$$
\mathcal{E} = \{u(t) = \sum_i F_i \mathbf{1}_{(t_i, t_{i+1}]}(t), t1 < \dots < t_{n+1}, t_i \in T, F_i \in \mathcal{F}_{t_i} \text{square integrable} \}
$$

The Ito integral w.r.t. Brownian motion is

$$
\int_T u(t) dB(t) = \sum_i F_i(B(t_{i+1}) - B(t_i))
$$

**Definition of multiple Wiener-Ito integral** This is a multiple dimensional integral

$$
\int_{T^n} f(t_1, t_2, \dots, t_n) dB(t_1)dB(t_2)\dots dB(t_n)
$$

$$I_n(f) = \sum_{i_1, \dots, i_n} a_{i_1 \dots i_n} \xi_{i_1} \dots \xi_{i_n} $$

Looking at this equation, one may think that order of $i_1,\dots, i_n$ may affect to $a$ but not for the product of $\xi$. So, the symmetrized version is defined (because $I_n$ is linear as well)

$$
\tilde{f}(t_1, \dots, t_n) = \frac{1}{n!} \sum_{\sigma \in \mathcal{S}_n} f(t_{\sigma(1)}, \dots, t_{\sigma(n)})
$$

with $\mathcal{S}_n$ is the set of all permutations. Because $dt_1...dt_n$ is symmetry, we have

$$
\int_{T^n} |f(t_1, \dots, t_n)|^2dt_1...dt_n = \int_{T^n}f(t_{\sigma(1)}, \dots, t_{\sigma(n)}) dt_1...dt_n
$$

Using the triangle inequality, we have

$$
\lvert\lvert \tilde{f}\rvert\rvert_{L^2(T^n)} \leq \frac{1}{n!} \sum_{\sigma \in \mathcal{S}_n} \lvert\lvert {f}\rvert\rvert_{L^2(T^n)} = \lvert\lvert {f}\rvert\rvert_{L^2(T^n)}
$$

We can say the Wiener-Ito integral of $f$ and $\tilde{f}$ are the same

<br><br> <strong>Lemma</strong> <i>If $f \in \mathcal{E}_n$, elementary process, then </i> $I_n(f) = I_n(\tilde{f})$
<br><br>
This is quite easy to see if considering the symmetry of $\prod_i (B(t_i^{(2)} - B(t_i^{(1)}))$. The permutation version of this will have the same result.

Next, the following is the orthogonal property.

<br><br> <strong>Lemma</strong> <i>If $f, g \in \mathcal{E}_n$, elementary process, then </i>

$$
\mathbb{E}[I_n(f)] = 0, \quad \quad \mathbb{E}[I_n(f)I_m(g)] = \begin{cases} 0, \quad& n \neq m \\
  n! \langle \tilde{f}, \tilde{g} \rangle_{L^2(T^n)}, &n=m \end{cases}
$$
<br><br>

The first expectation is straightforward because using definition of elementary process and expect of Brownian motion

The second expectation needs to be treated more carefully. By the definition, this product will be the product of two summations, only the case that $\mathbb{E}[\xi^2] = \Delta t$ (basic Brownian motion property) remains, explaining when $n\neq m$, the expectation vanishes. 

Continuing with defining the Wiener-Ito integral on $L^2{T^n}$ instead of elemetary process space, the general steps are based on a sequence of $\{f_k\} \in \mathcal{E}_n$ converging to $f \in L^2{T^n}$. This leads to the convergence in probability of expecation of $I_n(f)$. 


## The Malliavin derivative and its adjoint

### Definition and properties

**Goal**: Rigorously define differentation w.r.t. white noise. 

In Wiener process, we usually encounter that its derivative is a Gaussian noise, $\xi_i(t) = \frac{dW_i}{dt}$

The new operator $D_t^{(i)}$ takes derivative of a random variable w.r.t to $\xi_i(t)$. We may expect this operator works as

$$
D_t^{(i)} W(h) = h_i(t)
$$

It is because 

$$
W(h) = \sum_{i=1}^m \int_0^\infty h_i(t)\xi_i(t) dt.
$$

We also expect the chain rules


$$
D_t^{(i)} F(X1, \dots, X_n) = \sum_{k=1}^n \partial_k F(X_1, \dots, X_n)D_t^{(i)}X_k
$$

In fact, the definition of $\mathscr{D}F$ can be interpreted as a directional derivative

$$
\langle DF, h \rangle = \lim_{\epsilon \to 0}\frac{1}{\epsilon} (F(W(h_1) + \epsilon \langle h_1, h\rangle, \dots, W(h_n) + \epsilon \langle h_n, h\rangle) - F)
$$



<br><br>
<strong> Proposition </strong> (Integration by parts) <i>For every $X$, $h$, one has the identity</i>
$$
\mathbb{E}[\langle {D}X, h\rangle_H] = \mathbb{E}[XW(h)]
$$
<br><br>
**Proof** It is okay to consider only the case $\lvert\lvert h \rvert\rvert_H=1$.Suppose orthonormal basis $\{e_1, \dots, e_n\}$ of $H$ such that $h=e_1, F = f(W(e_1), \dots, W(e_n))$

Given $\phi(x)$ denoting standard normal distribtution, we have

$$\mathbb{E}[\langle DF, h \rangle_H] = \int \partial_1 f(x)\phi(x)dx = \int f(x)\phi(x)x_1 dx = \mathbb{E}[FW(e_1)]= \mathbb{E}[FW(h)]$$
The second equation used integration by part. 

The following result uses $D(GF) = (DG)F + G(DF)$ (something like chain rule).

<br><br>
<strong> Lemma </strong> <i>Let $F, G \in \mathcal{S}$ and $h \in H$</i>

$$
\mathbb{E}[G\langle {D}F, h\rangle_H] = -\mathbb{E}[F\langle DG, h \rangle_H] + \mathbb{E}[FGW(h)]
$$
<br><br>
### The derivative operator in the white noise case
Consider the case of one-dimensional Brownian motion $B(t), t \in T = [a, b],  H = L^2(T)$. The functional $W(h) = \int_a^b h(s) dB(s)$

**Proposition** 
$F = \sum_{n=0}^\infty I_n(f_n(\cdot, t))$
$$
D_tF = \sum_{n=1}^\infty n I_{n-1}(f_n(\cdot, t)). 
$$

**Proof** We also start with elementary process where $f_n \in \mathcal{E}_n$ symmetric. 
Consider a really simple case $F = I_n(f_n)$

## Applications
This part will focus on how to use Malliavin calculus in mathematical finance. Again, the main concern when I read this section is that the benefit of using Malliavin calculus over Ito calculus.

### Pricing and hedging financial options

### The Black-Scholes model

### Pricing and hedging options in the Black-Scholes model

### Sensibility with respect to the parameters: the greeks

### Application of the Clark-Ocone formula in hedging

