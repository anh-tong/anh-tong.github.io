<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Anh  Tong


  | Numerical methods for SDEs

</title>
<meta name="description" content="Anh Tong personal homepage
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2021/numerical-sde/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Numerical methods for SDEs",
      "description": "Study some approximation methods in solving SDEs, i.e., Euler-Maruyama method, Milstein's method,...",
      "published": "November 2, 2021",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Anh</span>   Tong
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/experience/">
                Experience
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Numerical methods for SDEs</h1>
        <p>Study some approximation methods in solving SDEs, i.e., Euler-Maruyama method, Milstein's method,...</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p>This post summarizes some well-known numerical methods in solving stochastic differential equations (SDEs). Partial differential equations (PDEs) in general, or SDEs specifically selfdom lead to exact solution so we need to seek approximation solutions. Compared to traditional PDEs, solving SDEs requires to handle the randomness associated to the equations. In most of SDEs, we often encounter white noise or derivative of Brownian motion. Different methods will tackle with this randomness differently.</p>

<h2 id="euler-maruyama-method">Euler-Maruyama method</h2>

<p>At the heart of numerical methods for differential equations, the key idea is <em>discretization</em>. The derivative is approximated as</p>

\[\frac{dx}{dt}(t) \approx \frac{x(t + \Delta t) - x(t)}{\Delta t}\]

<p>A simple ordinary differential equation will be converted as</p>

\[\frac{dx}{dt} = f(x, t) \quad \Rightarrow \quad x(t + \Delta t) = x(t) + f(x, t) \Delta\]

<p>Assume that we start at $x(0)$, we have to go through many step $\Delta t$ to reach $x(T)$ at time $T$. In this case, the discretization task is simple by partitioning the time interval. For 2-dimensional or 3-dimensional PDEs, there are discretization methods, for example, finite element methods, designed specifically to create meshes over domain spaces.</p>

<p>Now, consider a SDE</p>

\[dX_t = f(X_t, t) dt + g(X_t, t) dW_t\]

<p>The Euler-Maruyama method gives us the approximation as</p>

\[X(t + \Delta t) = X(t) + f(X, t) \Delta t + g(X, t) \Delta W_t\]

<p>Since $W_t$ is a Wiener process or a Brownian motion, the difference between to time steps is distributed as</p>

\[\Delta W_t = W_{t + \Delta t} - W(t) \sim \mathcal{N}(0, \Delta t)\]

<p>So $\Delta W_t$ will be sampled during solving the SDE and computed by $\eta \sqrt{\Delta t}$ as we sample $\eta$ from a standard normal distribution $\mathcal{N}(0, 1)$.</p>

<p>The final form of sampling process for $X$ is</p>

\[X(t + \Delta t) = X(t) + f(X, t) \Delta t +  \eta g(X, t)  \sqrt{\Delta t}, \quad \eta \sim \mathcal{N}(0, 1).\]

<p><strong>Approximation Error Analysis</strong></p>

<p>Because of the way of discretizing, the Euler method approximates the real solution with accuracy $\mathcal{O}(\Delta t)$.</p>

<p>To assess the performance of an approximate method, one considers two types of convergence</p>

<ul>
  <li>Strong Convergence: $\mathbb{E}[\lvert x - X \rvert] \leq C \Delta t^\gamma$, using the factor $\gamma$ to judge the approximation accuracy.</li>
  <li>Weak Convergence: difference of first-order estimate $\lvert\mathbb{E}[x] - \mathbb{E}[X]\rvert \leq C \Delta t^\beta$.</li>
</ul>

<p>The Euler-Maruyama has a strong convergence order of $1/2$ and a weak convergence of order $1$ which is an extremely slow convergence.</p>

<h2 id="milsteins-method">Milstein’s method</h2>

<p>The Milstein’s method is to improve the Euler-Maruyama by taking into account Ito’s lemma.</p>

\[X(t + \Delta t) = X(t) + \left(f(X,t) - \frac{1}{2} gg_x (X, t)\right) \Delta t + \sqrt{\Delta t} g(X, t)\eta + \frac{\Delta t}{2} gg_x(X, t)\eta^2\]

<p>Why is there a new term $gg_x(X, t) = g(X,t)\frac{\partial g}{\partial x}(X, t)$? This comes from the fact that $(dW_t)^2 = dt$ and Ito’s lemma.</p>

<p><strong>Explanation with geometric Brownian motion</strong> 
Given that $dX_t = \mu X dt + \sigma X dW_t$, Ito’s lemma for $\ln X_t$ is</p>

\[d \ln X_t = \left(\mu - \frac{1}{2}\sigma^2\right)dt + \sigma dW_t\]

<p>For an interval $[t, t + \Delta t]$, the approximated solution is derived as</p>

\[\begin{aligned}
X_{t + \Delta t} = &amp; X_t \exp \left \{\int_t^{t + \Delta t}\left(\mu - \frac{1}{2}\sigma^2\right)du + \int_t^{t + \Delta t}\sigma dW_u \right\} \\
\approx &amp; X_t \left( 1 + \left(\mu - \frac{1}{2}\sigma^2\right)\Delta t + \sigma \Delta W_t + \frac{1}{2} \sigma^2 (\Delta W_t)^2 + \mathcal{O}(t^2)\right)
\end{aligned}\]

<p>The last equation is the Talor expansion of $\exp(\cdot)$ where $\Delta t$ goes up to order $1$, $\Delta W_t$ goes up to order $2$. In this case, $f(X, t) = \mu X, g(X,t) = \sigma X$, resulting in $g g_x(X, t) = X\sigma^2$.</p>

<p>Although this is the derivation for a specific case of geometric Brownian motion, the general explanation is possible.</p>

<p><strong>Explanation for the general case</strong> Now, going back considering the case</p>

\[dX_t = f(X_t, t) dt + g(X_t, t) dW_t\]

<p>Assume we work on a small interval $[0, h]$. And on this particular interval, $f(X_t, t) = f(X_0, 0), g(X_t,t) = g(X_0, 0)$. This leads to</p>

\[X_t \approx X_0 + f(X_0, 0)t + g(X_0, 0) dW_t = X_0 + g(X_0, 0) W_t + \mathcal{O}(h)\]

<p>On the other hand,</p>

\[\begin{aligned}
g(X_t, t) = &amp; g(X_0, 0) + g_x(X_0, 0)(X_t - X_)) + \mathcal{O}(h) \\
= &amp; g(X_0, 0) + g_x(X_0, 0)g(X_0, 0)W_t + \mathcal{O}(h)
\end{aligned}\]

<p>We can then obtain</p>

\[S_h = S_0 + f(X_0, 0)h + g(X_0, 0)W_h + gg_x(X_0,0)\int_0^hW_tdW_t + \mathcal{O}(h^{3/2})\]

<p>Now, Ito calulus is used to derive $\int_0^h W_t dW_t = \frac{1}{2}W_h^2 - \frac{1}{2}h$. To this end, we can obtain the Milstein formular.</p>

<p>Both strong and weak convergence is of order $1$ which is better than the Euler-Maruyama method. See <a href="https://hautahi.com/sde_simulation">this</a> for a simulation comparison between the two methods</p>

<h2 id="kps-method">KPS method</h2>

<p>For a better convergence, one may look for a stronger convergence. The following is the KPS method. Advanced method can be found like <a href="https://www.sciencedirect.com/science/article/abs/pii/S016892749600027X">high-order Runge-Kutta methods</a> which, however, will not discuss here.</p>

<p>The update for KPS method follows</p>

\[\begin{aligned}
X_{t + \Delta t} = &amp; X_t + f\Delta t + g \Delta W_t + \frac{1}{2} gg_x ((\Delta W_t)^2 - \Delta t) \\
&amp;+ gf_x \Delta U_t + \frac{1}{2}\left(ff_x + \frac{1}{2}g^2f_{xx}\right)\Delta t^2\\
&amp; + \left(fg_x + \frac{1}{2}g^2gg_{xx}\right)(\Delta W_t \delta t - \Delta U_t) \\
&amp; + \frac{1}{2}g(gg_x)_x\left(\frac{1}{3}(\Delta W_t)^3 -\Delta t\right) \Delta W_t
\end{aligned}\]

<p>where $\Delta W_t = \sqrt{\Delta t}\eta, \eta \sim \mathcal{N}(0,1)$ and $\Delta U_t = \int_{t}^{t + \Delta t} \int_t^s dW_s ds = \frac{1}{3}\Delta t^3 \lambda, \lambda \sim \mathcal{N}(0,1)$.</p>

<p>Phew! The formula is complicated. Why am I bothering writing this down?</p>
<h2 id="adaptive-timestep">Adaptive timestep</h2>
<p>If there are adaptive learning rate in machine learning, numerical methods of differential equations have ways to pick adaptive timestep instead of fixed one. This analogy does not make sense since the approach is completely different.</p>

<p>The procedure consists of two steps:</p>

<ol>
  <li>Propose a step size and estimate the error. Here, estimating error may consider the inherence of noise</li>
  <li>Accept or reject the step size according to some criteria.</li>
</ol>

<p>A complete description is in <a href="https://www.aimsciences.org/article/doi/10.3934/dcdsb.2017133">this reference<a>.</a></a></p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Anh  Tong.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



    

  </body>

  <d-bibliography src="/assets/bibliography/">
  </d-bibliography>

  

</html>
