<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Anh  Tong


  | Malliavin Calculus

</title>
<meta name="description" content="Anh Tong personal homepage
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2021/malliavin_calculus/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Malliavin Calculus",
      "description": "(in progress) Some background of Mallivian Calculus",
      "published": "November 10, 2021",
      "authors": [
        
        {
          "author": "Anh Tong",
          "authorURL": "https://anh-tong.github.io/",
          "affiliations": [
            {
              "name": "KAIST",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Anh</span>   Tong
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/experience/">
                Experience
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Malliavin Calculus</h1>
        <p>(in progress) Some background of Mallivian Calculus</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-toc></d-toc>

<h2 id="introduction">Introduction</h2>

<p>This is a long post distilling some concepts of Malliavin Calculus and based on the <a href="http://hairer.org/notes/Malliavin.pdf">lecture note</a> of <a href="https://en.wikipedia.org/wiki/Martin_Hairer"> Martin Hairer</a>.</p>

<p><strong>Motivation</strong> Malliavin calculus is a modern tool tackling with differentiating random variable defined on a Gaussian probability space w.r.t. the underlying noise.</p>

<p>At the moment, I feel like White Noise Theory and Malliavian calculus share some similarity. They surely complement each other but I do not comprehend the difference between them, for example, what one can do but other cannot.
I also plan to get to know more some background of <a href="http://www.hairer.org/notes/RoughPaths.pdf">rough path</a> but this will be in another post. </p>
<!-- <aside>Some text in an aside, margin notes, etc...</aside> -->

<p>Stochastic analysis centers around stochastic differential equations, i.e.,</p>

\[dX_t = V_0(X_t)dt + \sum_{i=1}^m V_i(X_t) \circ dW_i(t)\]

<p>where $\circ dW_t$ denotes Straonovich integration. In this equation, Hairer considers multiple noise parts.</p>

<h2 id="white-noise-and-wiener-chaos">White noise and Wiener chaos</h2>

<p>This section concerns the definition of white noise under functional representation. Wiener chaos gives the decomposition form of white noise in which we will find somewhat similar to Fourier analysis or <a href="Sobolev space">Sobolev space</a>.</p>

<h3 id="definition-of-white-noise">Definition of white noise</h3>

<p>Now, letâ€™s talk about spaces we will work on</p>

<ul>
  <li>$H = L^2(\mathbb{R}_+, \mathbb{R}^m)$: a real and separatable Hilbert space</li>
  <li>$L^2(\Omega, \mathbb{P})$: for some probability space $(\Omega, \mathbb{P})$</li>
</ul>

<p>White noise is linear isometry<d-footnote>linear map preserving distance</d-footnote> $W: H \to L^2(\Omega, \mathbb{P})$ such that the ouput $W(h)$ is a real-valued Gaussian variable or</p>

\[\mathbb{E}[W(h)] = 0, \qquad \mathbb{E}[W(h)W(g)] = \langle h, g \rangle_H.\]

<p>The above is just the definition. How to establish such map will be shown next.</p>

<p><strong>Orthonormal basis</strong> Here, we define</p>
<ul>
  <li>a sequence of i.i.d. normal random variable ${\xi_n}_{n\geq 0}$</li>
  <li>an orthonormal basis ${e_n}_{n \geq 0}$ of $H$.</li>
</ul>

<p>When representing $h = \sum_n h_n e_n \in H$, we construct $W(h)=\sum_n h_n \xi_n$. The normal random variable $\xi_n$ is now can rewrite in the functional form form $\xi_n = W(e_n)$.</p>

<p>In the lecture, $m$-dimensional Wiener process is defined by using funtion $\mathbf{1}_{[0,t)}^{(i)}$</p>

\[(\mathbf{1}_{[0,t)}^{(i)})_j(s) = \begin{cases}
   1 &amp;\text{if } s \in [0, t) \text{ and } j=i \\
   0 &amp;\text{otherwise } 
\end{cases}\]

<p>Note that 1-dimensional case is easy to show, but for now, still follow the setup in the lecture.</p>

<p>The $i$-th dimension of Wiener process is defined as $W_i(t) = W(\mathbf{1}_{[0,t)}^{(i)})$ where we can check the covariance</p>

\[\mathbb{E}[W_i(t)W_j(s)] = \langle \mathbf{1}_{[0,t)}^{(i)}, \mathbf{1}_{[0,s)}^{(j)} \rangle = \delta_{ij}(t \wedge s).\]

<p>For arbitrary $h$, $W(h)$ can represent as (Wiener-Ito integral)</p>

\[W(h) = \sum_{i=1}^m \int_0^\infty h_i(s) dW_i(s)\]

<p>Note that we may need to clearly differentiate between $W(h)$ and $W_i(s)$.</p>

<h3 id="representation-using-hermite-polynomials">Representation using Hermite polynomials</h3>

<p>There are several formulation of Hermite polynomial</p>

<ul>
  <li>Recursive $H^\prime_n(x) = n H_{n-1}(x)$</li>
  <li>A different recursive $H_{n + 1}(x) = xH_n(x) - H^\prime_n(x)$,</li>
  <li>Explicit representation $H_n(x) = \exp\left(-\frac{D^2}{2}\right) x^n$, where $D$ is the differentiation w.r.t. to $x$, $\exp(\cdot)$ is defined in the Taylor expansion sense.</li>
</ul>

<p>Some properties of Hermite polynomials:</p>

<ul>
  <li>$\mathbb{E}[H_n(X)] = 0$: $H_n(X)$ has zero-mean when $X \sim \mathcal{N}(0,1)$</li>
  <li>Identity</li>
</ul>

\[\begin{aligned}
\int H_n(x) H_m(x) e^{-x^2/2} dx  = &amp; 
\frac{1}{n + 1} \int H'_{n + 1}(x) H_m(x) e^{-x^2/2} dx \\
 = &amp; \frac{1}{n + 1} \int H_{n + 1}(x) (xH_m(x) - H'm(x)) e^{-x^2/2} dx \\
 = &amp; \frac{1}{n + 1} \int H_{n + 1}(x) H_{m + 1}(x) e^{-x^2/2} dx
\end{aligned}\]

<p>This recursive leads to $\mathbb{E}[H_n(X)H_m(X)] = n! \delta_{n.m}$</p>

<p>Letâ€™s define linear subspaces of $L^2(\Omega, \mathbb{P})$ as</p>

\[\mathcal{H} = \{H_n(W(h)), h \in H, \lvert\lvert h \rvert\rvert_H = 1\}\]

<p>We have the following decomposition
<br /><br /><strong>Theorem</strong> <i> Let $\mathcal{F}$ be the $\sigma$-algebra generated by $W$. Then,</i></p>

\[L^2(\Omega, \mathcal{F}, \mathbb{P}) = \bigoplus_{n=0}^\infty \mathcal{H}_n\]

<p><br /><br /></p>

<p><strong>Proof</strong> Let $X \in L^2(\Omega, \mathcal{F}, \mathbb{P})$ be orthogonal to $\mathcal{H}_n$ <em>for all</em> $n$.</p>

\[\mathbb{E}[XH_n(W(h))] = 0, \forall n \quad \Rightarrow \quad \mathbb{E}[X\exp(W(h))] = 0\]

<p>We need to show that $X=0$. Splitting $X = X^+ - X^-$, and define the following measures</p>

\[\nu^{+,-} = \mathbb{E}[X^{+,-} \mathbf{1}_B(W(h_1), \dots, W(h_m))], \quad B \in \mathcal{B}(\mathbb{R}^m)\]

<p>Applying Laplace transform for $\nu$, we deduce</p>

\[\varphi_{\nu^{+,-}}(\lambda) = \int \exp(\lambda \cdot x) \nu^{+,-}(dx) = \mathbb{E}[X^{+,-}\exp(\sum_i \lambda_i W(h_i))] = 0\]

<p>As the Laplace is zero, then the measure is zero. Thus, $\mathbb{E}[X\mathbb{1}_F] = 0, \forall F \in \mathcal{F}$. Therefore, $X=0$ and we can conclude the proof.</p>

<h3 id="representation-using-multiple-stochastic-integrals">Representation using multiple stochastic integrals</h3>

<p>This section defines multiple Wiener-Ito integral w.r.t. Brownian motion. With this definition we can lead to a similar decomposition like the representation of Hermite polynomials presented above.</p>

<p>Consider</p>

<ul>
  <li>measure space $(T, \mathcal{B}, \mu)$</li>
  <li>one-dimensional Brownian motion $B(t), t \in T = [a, b]$</li>
  <li>Functional space $H=L^2([a, b], \mathbb{R})$</li>
  <li>Functional representation $W(h) = \int_a^b h(s) dB(s)$</li>
</ul>

<p>Like traditional stochastic calculus, this tries to set up a corner stone with elementary process</p>

\[\mathcal{E} = \{u(t) = \sum_i F_i \mathbf{1}_{(t_i, t_{i+1}]}(t), t1 &lt; \dots &lt; t_{n+1}, t_i \in T, F_i \in \mathcal{F}_{t_i} \text{square integrable} \}\]

<p>The Ito integral w.r.t. Brownian motion is</p>

\[\int_T u(t) dB(t) = \sum_i F_i(B(t_{i+1}) - B(t_i))\]

<p><strong>Definition of multiple Wiener-Ito integral</strong> This is a multiple dimensional integral</p>

\[\int_{T^n} f(t_1, t_2, \dots, t_n) dB(t_1)dB(t_2)\dots dB(t_n)\]

\[I_n(f) = \sum_{i_1, \dots, i_n} a_{i_1 \dots i_n} \xi_{i_1} \dots \xi_{i_n}\]

<p>Looking at this equation, one may think that order of $i_1,\dots, i_n$ may affect to $a$ but not for the product of $\xi$. So, the symmetrized version is defined (because $I_n$ is linear as well)</p>

\[\tilde{f}(t_1, \dots, t_n) = \frac{1}{n!} \sum_{\sigma \in \mathcal{S}_n} f(t_{\sigma(1)}, \dots, t_{\sigma(n)})\]

<p>with $\mathcal{S}_n$ is the set of all permutations. Because $dt_1â€¦dt_n$ is symmetry, we have</p>

\[\int_{T^n} |f(t_1, \dots, t_n)|^2dt_1...dt_n = \int_{T^n}f(t_{\sigma(1)}, \dots, t_{\sigma(n)}) dt_1...dt_n\]

<p>Using the triangle inequality, we have</p>

\[\lvert\lvert \tilde{f}\rvert\rvert_{L^2(T^n)} \leq \frac{1}{n!} \sum_{\sigma \in \mathcal{S}_n} \lvert\lvert {f}\rvert\rvert_{L^2(T^n)} = \lvert\lvert {f}\rvert\rvert_{L^2(T^n)}\]

<p>We can say the Wiener-Ito integral of $f$ and $\tilde{f}$ are the same</p>

<p><br /><br /> <strong>Lemma</strong> <i>If $f \in \mathcal{E}_n$, elementary process, then </i> $I_n(f) = I_n(\tilde{f})$
<br /><br />
This is quite easy to see if considering the symmetry of $\prod_i (B(t_i^{(2)} - B(t_i^{(1)}))$. The permutation version of this will have the same result.</p>

<p>Next, the following is the orthogonal property.</p>

<p><br /><br /> <strong>Lemma</strong> <i>If $f, g \in \mathcal{E}_n$, elementary process, then </i></p>

<p>\(\mathbb{E}[I_n(f)] = 0, \quad \quad \mathbb{E}[I_n(f)I_m(g)] = \begin{cases} 0, \quad&amp; n \neq m \\
  n! \langle \tilde{f}, \tilde{g} \rangle_{L^2(T^n)}, &amp;n=m \end{cases}\)
<br /><br /></p>

<p>The first expectation is straightforward because using definition of elementary process and expect of Brownian motion</p>

<p>The second expectation needs to be treated more carefully. By the definition, this product will be the product of two summations, only the case that $\mathbb{E}[\xi^2] = \Delta t$ (basic Brownian motion property) remains, explaining when $n\neq m$, the expectation vanishes.</p>

<p>Continuing with defining the Wiener-Ito integral on $L^2{T^n}$ instead of elemetary process space, the general steps are based on a sequence of ${f_k} \in \mathcal{E}_n$ converging to $f \in L^2{T^n}$. This leads to the convergence in probability of expecation of $I_n(f)$.</p>

<h2 id="the-malliavin-derivative">The Malliavin derivative</h2>

<h3 id="definition-and-properties">Definition and properties</h3>

<p><strong>Goal</strong>: Rigorously define differentation w.r.t. white noise.</p>

<p>In Wiener process, we usually encounter that its derivative is a Gaussian noise, $\xi_i(t) = \frac{dW_i}{dt}$</p>

<p>The new operator $D_t^{(i)}$ takes derivative of a random variable w.r.t to $\xi_i(t)$. We may expect this operator works as</p>

\[D_t^{(i)} W(h) = h_i(t)\]

<p>It is because</p>

\[W(h) = \sum_{i=1}^m \int_0^\infty h_i(t)\xi_i(t) dt.\]

<p>We also expect the chain rules</p>

\[D_t^{(i)} F(X1, \dots, X_n) = \sum_{k=1}^n \partial_k F(X_1, \dots, X_n)D_t^{(i)}X_k\]

<p>In fact, the definition of $\mathscr{D}F$ can be interpreted as a directional derivative</p>

\[\langle DF, h \rangle = \lim_{\epsilon \to 0}\frac{1}{\epsilon} (F(W(h_1) + \epsilon \langle h_1, h\rangle, \dots, W(h_n) + \epsilon \langle h_n, h\rangle) - F)\]

<p><br /><br />
<strong> Proposition </strong> (Integration by parts) <i>For every $X$, $h$, one has the identity</i>
\(\mathbb{E}[\langle {D}X, h\rangle_H] = \mathbb{E}[XW(h)]\)
<br /><br />
<strong>Proof</strong> It is okay to consider only the case $\lvert\lvert h \rvert\rvert_H=1$.Suppose orthonormal basis ${e_1, \dots, e_n}$ of $H$ such that $h=e_1, F = f(W(e_1), \dots, W(e_n))$</p>

<p>Given $\phi(x)$ denoting standard normal distribtution, we have</p>

<p>\(\mathbb{E}[\langle DF, h \rangle_H] = \int \partial_1 f(x)\phi(x)dx = \int f(x)\phi(x)x_1 dx = \mathbb{E}[FW(e_1)]= \mathbb{E}[FW(h)]\)</p>
<p>The second equation used integration by part.</p>
<aside>Because $f(x)$ grows linearly so $0 = \exp(-x^2/2)f(x)\rvert_{-\infty}^{\infty} = \int \partial f(x) \exp(-x^2/2) dx + \int f(x)\partial \exp(-x^2/2) dx $</aside>

<p>The following result uses $D(GF) = (DG)F + G(DF)$ (something like chain rule).</p>

<p><br /><br />
<strong> Lemma </strong> <i>Let $F, G \in \mathcal{S}$ and $h \in H$</i></p>

<p>\(\mathbb{E}[G\langle {D}F, h\rangle_H] = -\mathbb{E}[F\langle DG, h \rangle_H] + \mathbb{E}[FGW(h)]\)
<br /><br /></p>

<p><strong>Proposition</strong> [Chain rule] <i> Let $g: \mathbb{R}^d \to \mathbb{R}$ be a function in $\mathcal{C}^1$ with bounded partial deriviatives. Let $p\geq 1$ and $F = (F^1, \dots, F^d), F^i \in \mathbb{D}^{1,d}$. Then $g(F) \in \mathbb{D}^{1, p}$ and
</i></p>

\[D(g(F)) = \sum_{i=1}^d \partial_i g(F)DF^i\]

<h3 id="the-derivative-operator-in-the-white-noise-case">The derivative operator in the white noise case</h3>
<p>Consider the case of one-dimensional Brownian motion $B(t), t \in T = [a, b],  H = L^2(T)$. The functional $W(h) = \int_a^b h(s) dB(s)$</p>

<p><strong>Proposition</strong> 
$F = \sum_{n=0}^\infty I_n(f_n(\cdot, t))$
\(D_tF = \sum_{n=1}^\infty n I_{n-1}(f_n(\cdot, t)).\)</p>

<p><strong>Proof</strong> We also start with elementary process where $f_n \in \mathcal{E}_n$ symmetric. 
Consider a really simple case $F = I_n(f_n)$</p>

<p><strong>Proposition</strong> <i>Let $g: \mathbb{R}^d \to R$ be a Lipschitz function ($\lvert g(x) - g(y)\rvert  \leq K \lvert\lvert x- y\rvert\rvert$). Suppose $F = (F^1, \dots, F^d)$ is a random vector such that $F^i \in \mathbb{D}^{1,2}$. Then $g(F) \in \mathbb{D}^{1,2}$ and there exists a random vector $G=(G_1, \dots, G_d)$ such that</i></p>

\[D(g(F)) = \sum_{i=1}^d G_i DF^i.\]

<h2 id="divergence-operator">Divergence operator</h2>

<p>In short, divergence operator is defined as the dual (adjoint) of the derivative operator defined in the previous section.</p>

<h3 id="definition-of-divergence-operator">Definition of divergence operator</h3>

<p>The divergence operator is denoted as $\delta$ which is unbounded, $\delta: L^2(\Omega; H) \to L^2(\Omega)$, satisfying</p>

<ul>
  <li>
    <p>The domain of $\delta$, $\text{Dom} \delta$, contains $u \in L^2(\Omega; H)$ such that $\lvert \mathbb{E}[\langle DF , u \rangle_H] \rvert \leq c_u \lvert \lvert F \rvert\rvert_{L^2(\Omega)}$</p>
  </li>
  <li>
    <p><strong>Duality relation</strong>: $\mathbb{E}[F\delta(u)] = \mathbb{E}[\langle DF, u \rangle_H]$</p>
  </li>
</ul>

<p><strong>Proposition</strong>[Properties of divergence] <i></i></p>
<ul>
  <li>If $u \in \text{Dom}(\delta)$, then $\mathbb{E}[\delta(u)] = 0$</li>
  <li>Divergence operator is linear and closed under $\text{Dom} \delta$</li>
  <li>$\delta(u) = \sum_{j=1}^n F_j W(h_j) - \sum_{j=1}^n \langle DF_j, h_j \rangle_H$</li>
  <li>$\langle D(\delta(u)), h\rangle_H = \langle u, h \rangle_H + \delta\left( \sum_j \langle DF_j, h\rangle_H h_j \right)$
&lt;/i&gt;&lt;aside&gt;To prove the third point, we may use the integral by parts of the derivative operator&lt;/aside&gt;</li>
</ul>

<h3 id="the-skorohod-integral">The Skorohod integral</h3>

<p>This part will consider the restricted case which is Brownian motion. This makes the divergence $\delta(u)$ now is the Skorohod integral.</p>

<p>Consider the Wiener chaos expansion</p>

\[u(t) = \sum_n I_n(f_n(\cdot, t))\]

<p>The Skorohod integral will be represented as</p>

\[\delta(u) = \sum_{n=0}^\infty I_{n+1}(\tilde{f}_n)\]

<p>converging in $L^2(\Omega)$ where</p>

<p>$$
\tilde{f}_n(t_1, \dots, t_n, t) = \frac{1}{n+1} (f_n(t_1, \dots, t_n, t) + \sum_{i=1}^n f_n(t_1, \dots, t_{i-1}, t, t_{i+1}, \dots, t_n, t_i))
$$
</p>
<aside> The last term simply just is the exchange between $t$ and $t_i$</aside>

<p><strong>Proposition</strong>[Skorohod integral is Ito integral] $\delta(u)$ coincides with the Ito integral w.r.t. Brownian mtion, that is</p>

\[\delta(u) = \int_{a}^b u(s)dB(s)\]

<p><strong>Proof</strong> Consider an elementary adapted process</p>

\[u_t = \sum_j F_j \mathbf{1}_{(t_j, t_{j+1})}(t)\]

<p>Now looking at each of component in the sum</p>

\[\delta(F_j \mathbf{1}_{(t_j, t_{j+1})}(\cdot)) = F_j \delta(\mathbf{1}_{(t_j, t_{j+1})}(\cdot)) - \int_t D_t F_j \mathbf{1}_{(t_j, t_{j+1})}(t) dt = F_j(B(t_{j+1}) - B(t_j))\]

<h3 id="the-clark-ocone-formula">The Clark-Ocone formula</h3>

<p>Given $F$, exist $u$ such that
\(F = \mathbb{E}[F] + \int_0^\infty u(t)dB(t)\)</p>

<p>This result says that a stochastic process can be represented by its mean which is a deterministic part and a randomness part.</p>

<p><strong>Proof</strong> First, consider <em>zero-mean</em> integrable random variable $G$ that is orthogonal to all stochastic integrals $\int_{\mathbb{R}_+} u(t) dB(t)$.</p>

<p>Let $M_u(t) = \exp(\int_0^t u(s) dB(s) - \frac{1}{2}\int_0^t u^2(s)ds)$. By Itoâ€™s formula</p>

\[M_u(t) = M_u(0) + \int_0^t M_u(s) u(s) dB(s)\]

<p>Hence, such random variable $G$ is orthogonal to</p>

\[\mathcal{E}(h) = \exp\left(\int_0^\infty h(s) dB(s) - \frac{1}{2} \int_0^\infty h^2(s) ds\right)\]

<p>And ${ e^{W(h)}, h \in L^2(\mathbb{R}_+) }$ form a <em>total subset</em> of $L^2(\Omega)$, this leads to the desired conclusion.</p>

<h2 id="integration-by-parts-and-regularity">Integration by parts and regularity</h2>

<p>This section provide the foundation of the integration by parts in Malliavin calculus. This will help</p>

<h3 id="the-integration-by-parts-formula">The integration by parts formula</h3>

<p><strong>Proposition</strong> <i>Let $F, G$ be two random variables such that $F \in \mathbb{D}^{1,2}$ <d-footnote>domain of first-order derivative operator $D$ in $L^2$</d-footnote>. Let $u$ be an $H$-valued random variable such that $\langle DF, u \rangle_H$ a.s. and $Gu(\langle DF, u\rangle_H)^{-1} \in \text{Dom } \delta$. Then for any function $f\in \mathcal{C}^1$ with bounded derivatives, we have that</i></p>

\[\mathbb{E}[f'(F)G] = \mathbb{E}[f(F) H(F, G)],\]

<p><i>where $H(F, G) = \delta(Gu(\langle DF, u\rangle_H)^{-1})$ </i>.</p>

<h3 id="existence-and-smoothness-of-densities">Existence and smoothness of densities</h3>

<h3 id="hormanders-therem">Hormanderâ€™s therem</h3>

<p>The main focus of this theorem is to show there is a unique solutions for SDEs under some conditions.</p>

<p>Consider the following setup:</p>

<ul>
  <li>Brownian motion has $d$ dimensions: $B(t) = (B^1(t), \dots, B^d(t)), t\in [0, T]$</li>
  <li>Linear growth assumption</li>
</ul>

<p>Let $X(t)$ be the solution of $d$-dimensional systems of SDEs</p>

\[dX_i(t) = \sum_{j=1}^d \sigma_{ij}(X(t))dB^j(t) + b_i(X(t))dt, \quad X_i(0) = x_0^i, \quad i = 1,\dots, d\]

<p><strong>Theorem</strong> <i>There exists a unique continuous soltion and the following expectation is bounded</i></p>

\[\mathbb{E}\left[\sup_{0\leq t \leq T} \lvert X(t)\rvert^p\right] \leq C\]

<p><i>for any $p \geq 2$, where $C = C(p, T, K)&gt; 0$</i></p>

<p><strong>Theorem</strong> The derivative $D^j_rX_i(s))$</p>

\[D^j_rX_i(t)) = \sigma_{ij}(X(r)) + \sum_{k,l=1}^d \int_r^t \partial_k \sigma_{il}(X(s))D^j_s(X_k(s))dB^l(s) + \sum_{k=1}^d\int_r^t \partial_k b_i(X(s))ds\]

<p>Some notations:</p>

<ul>
  <li>Vector fields: $\sigma_j = \sum_{i=1}^d\sigma_{ij}(x)\frac{\partial}{\partial x_i}$, $b = \sum_{i=1}^d b_i(x)\frac{\partial}{\partial x_i}$</li>
  <li>Covariant derivative: $\sigma_j \nabla\sigma_k = \sum_{i,l=1}^d \sigma_{lj} \partial_l \sigma_{ik}\frac{\partial }{\partial x_i}$</li>
  <li>Lie bracket: $[\sigma_j,\sigma_k] = \sigma_j \nabla \sigma_k - \sigma_k \nabla \sigma_j$</li>
  <li>Define $\sigma_0 = b - \frac{1}{2}\sum_{i=1}^d\sigma_i \nabla \sigma_i$</li>
</ul>

<p>With these notation, the above SDE can be defined with a Stratonovich integral</p>

\[X(t) = X_0 + \sum_{j = 1}^d \int_0^t \sigma(X(s)) \circ dB^j(s) + \int_0^t \sigma_0(X(s))ds\]

<p><strong>Holder condition</strong> This is a vector space spanned by the vector filed</p>

\[\mathbf{(H)} = \text{span} \{\sigma_1, \dots, \sigma_d, [\sigma_i, \sigma_j], [\sigma_i, [\sigma_j, \sigma_k]]\}\]

<p><strong>Theorem</strong> <i>Assume that Hormanderâ€™s condition $\mathbf{(H)}$ holds and the coefficients of SDE are finitely differentiable. Then for any $t &gt; 0$, $X(t)$ has an infintely differentiable density.</i></p>

<p>The proof is based on the quadratic varation is large, then the semimartingale is small with an exponentially small probability.</p>

<h2 id="applications">Applications</h2>
<p>This part will focus on how to use Malliavin calculus in mathematical finance. Again, the main concern when I read this section is that the benefit of using Malliavin calculus over Ito calculus. The first three subsections contains some introductory background. The remaining subsections discussed the actual use of Malliavin calculus.</p>

<h3 id="pricing-and-hedging-financial-options">Pricing and hedging financial options</h3>

<p>This will give a brief introduction of options. An option is a contract, or right to buy (put) or sell (call) an amount of assets. Some terminologies related to this concept are</p>

<ul>
  <li>Strike price or exercise price $K$</li>
  <li>Maturity or exersize time $T$</li>
  <li>When performing exchange or executing what is written in contract, there is a fee $x$ for doing so. It is called option premium.</li>
</ul>

<p>There are two ways of exercising options</p>

<ul>
  <li>Europian options: exercising at maturity $T$</li>
  <li>American options: exercising at any time before maturity.</li>
</ul>

<p>The value of put or call are decided by</p>

\[C_T = \max(S_T - K, 0), \quad P_T = \max(K - S_T, 0)\]

<p>To further work with these values, we take into account their neural risk which involves analyzing their statistical estimation (mostly expectation) w.r.t. market randomness.</p>

<p>Two main questions that might be interesting are</p>

<ul>
  <li>pricing option: evaluate the price of an option at time $t=0$</li>
  <li>hedging option: evaluate the value of an option at maturity.</li>
</ul>

<h3 id="the-black-scholes-model">The Black-Scholes model</h3>

<p>The Black-Scholes model is very well-known in quantitative finance field, helping us to understand of the market dynamics. The downside, however, is that the model has restricted assumptions, therefore, usually is used for pedagodical purposes.</p>

\[dS_t = S_t \mu dt + S_t\sigma dB_t\]

<p>Ito calculus allows us to have a close-form solution for this SDE</p>

\[S_t = S_0 \exp(\mu t - \frac{\sigma^2}{2}t + \sigma B_t)\]

<p>Therefore,</p>

\[\mathbb{E}[S_t] = S_0\exp(\mu t), \quad \mathbb{E}[S_t^2] = S_0^2 \exp((2\mu + \sigma^2)t)\]

<h3 id="pricing-and-hedging-options-in-the-black-scholes-model">Pricing and hedging options in the Black-Scholes model</h3>

<p>There is an equivalence between the solution of Black-Scholes models and a partial diferential equation (PDE).</p>

<p><strong>Theorem</strong> <i> Let $h$ be a continuous function of at most linear growth. Assume that $v(t, y)$ is a regular solution of PDE</i></p>

\[\begin{cases}
\frac{1}{2}\sigma^2 y^2 \frac{\partial^2 v}{\partial y^2} + ry \frac{\partial v}{\partial y} + \frac{\partial v}{\partial t} - r v = 0
\\
v(T, y) = h(y)
\end{cases}\]

<p><i>There exists a portfolio with value $v(t, S_t)$ at time $t$ replicated flow $h(S_T)$. And the value of this hedging portfoliio is given by $\beta(t, S_t) = \frac{\partial v}{\partial y}(t, S_t)$. </i></p>

<p>Letâ€™s take a moment to think how to interpret this theorem. Looking at the boundary condition, this means that we may expect that at maturity $T$, the solution $v$ should agree with the function $h$. And the solution $v(t, y)$ on $(0, T)$ describes the dynamics of $v$ along the interval. This is the backward solution because we start at $T$ and go back to $0$.</p>

<p><strong>Proof</strong></p>
<p>Using Ito's formula to $v(t, S_t)$ </p>
<aside>like chain rule in traditional calculus but have additional second derivative. Also, $S_t$ should be semi-martingale</aside>

\[dv(t, S_t) = \frac{\partial v}{\partial t}(t, S_t) dt + \frac{\partial v}{\partial y}(t, S_t) dS_t + \frac{1}{2}\sigma^2S_t^2\frac{\partial^2 v}{\partial y^2}(t, S_t) dt\]

<p>The above is purely a mathematical derivation. On the hand, managing portfolio requires to</p>

\[dv(s, S_t) = v(t, S_t)rdt + \beta(t, S_t)(dS_t - rS_tdt)\]

<p>Picking $\beta(t, S_t) = \frac{\partial v}{\partial y}(t, S_t)$, the part with $dS_t$ vanishes, the remain will be reduced to</p>

\[\frac{\partial v}{\partial t}(t, S_t) + \frac{1}{2}\sigma^2S_t^2\frac{\partial^2 v}{\partial y^2}(t, S_t) = v(t, S_t)rdt - rS_t\frac{\partial v}{\partial y}(t, S_t)\]

<p>And we obtain the expect PDE.</p>

<h3 id="sensibility-with-respect-to-the-parameters-the-greeks">Sensibility with respect to the parameters: the greeks</h3>

<p>Consider the price of an option $V_0$ with strike $K$ and maturity $t$.</p>

<p>The most crucial parameters are $(x, r, \sigma, T, K)$</p>

<ul>
  <li>the premimum $x$</li>
  <li>the interest rate $r$</li>
  <li>the volatility $\sigma$</li>
</ul>

<p>People working in finance are interested in obtaining some quanities named after some characters in Greek alphabet:</p>

<ul>
  <li><strong>Delta:</strong> $\Delta = \frac{\partial V_0}{\partial x}$</li>
  <li><strong>Gamma:</strong> $\Gamma = \frac{\partial^2 V_0}{\partial^2 x}$</li>
  <li><strong>Vega:</strong> $\vartheta = \frac{\partial V_0}{\partial \sigma}$</li>
</ul>

<p>These Greeks will be computed using integration by parts of Mallivian calculus.</p>

<h3 id="application-of-the-clark-ocone-formula-in-hedging">Application of the Clark-Ocone formula in hedging</h3>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Anh  Tong.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



    
    <div class="post distill" id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname  = 'anh-tong-blog';
      var disqus_identifier = '/blog/2021/malliavin_calculus';
      var disqus_title      = "Malliavin Calculus";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  

  </body>

  <d-bibliography src="/assets/bibliography/">
  </d-bibliography>

  

</html>
