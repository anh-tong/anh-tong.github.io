<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Anh  Tong


  | Diffusion processes

</title>
<meta name="description" content="Anh Tong personal homepage
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2021/diffusion-process/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Diffusion processes",
      "description": "Note on diffusion processes",
      "published": "October 14, 2021",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Anh</span>   Tong
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/experience/">
                Experience
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Diffusion processes</h1>
        <p>Note on diffusion processes</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p>This note focuses on studying diffusion processes. The setting here is that we are solving stochastic differential equations (SDEs). The name “diffusions” is refered as the solutions to SDEs.</p>

<p>There is a recent direction in generative model (i.e., <a href="https://arxiv.org/abs/2011.13456"> scored-based generative modeling</a>) where the backbone is a diffusion process. Also, <a href="https://anh-tong.github.io/blog/2021/application_machine_learning/">the previous post</a> has discussed the importance of SDEs in Langevin optimization. So let’s dive in the main content.</p>

<h2 id="martingales-and-dynkins-formula">Martingales and Dynkin’s Formula</h2>

<p>Let $X(t)$ solve</p>

\[dX(t) = \mu(X(t), t) dt + \sigma(X(t),t)dB(t), \quad t \geq 0\]

<p>Define an operator $L_t$</p>

\[L_t f(x,t) = \frac{1}{2}\sigma^2(x,t) \frac{\partial^2 f}{\partial x^2} (x,t) + \mu(x, t) \frac{\partial f}{\partial x}(x,t)\]

<p>This is simply a shorthand version of <a href="https://en.wikipedia.org/wiki/It%C3%B4%27s_lemma">Ito’s formula </a> and we can write</p>

\[df(X(t),t) = \left(L_tf(X(t),t) + \frac{\partial f}{\partial t}(X(t),t)\right) dt + \frac{\partial f}{\partial x}(X(t), t)\sigma(X(t),t) dB(t)\]

<p>The first result presented here is that the following process is a martingale</p>

\[M_f(t) = f(X(t), t) - \int_0^t \left( L_u + \frac{\partial f}{ \partial t}\right)(X(u), u) du\]

<p>Of course, $X(t)$ and $f(x, t)$ have to satisfy a “reasonable” condition which will not be written in detail here.</p>

<p>The above equation makes sense when we use Ito’s lemma and $M_f(t) = \int_0^t \frac{\partial f(B(s), s)}{\partial x} dB(s)$ for the case $X(t) = B(t)$ (Brownian motion).</p>

<p>Given a function $f(x,t)$ which solves a PDE</p>

\[L_t f(x, t) + \frac{\partial f}{\partial t}(x, t) =0\]

<p>and we can say that $f(X(t), t)$ is martingale.</p>

<p>Now, we talk about the expectation. Dynkin’s formula will do so, and is obtained straightforward from the martingale property in the previous result. Formally, we have</p>

\[\mathbb{E}[f(X(t), t)] = f(X(0), 0) + \mathbb{E}\left[\int_0^t \left( L_u + \frac{\partial f}{ \partial t}\right)(X(u), u) du \right]\]

<p>This is also true if $t$ is replaced by a bounded stopping time $\tau$.</p>

<h2 id="calculation-of-expectations-and-pdes">Calculation of Expectations and PDEs</h2>

<p>This section provides a connection between computing the expectation of a diffusion process on the boundary and the corresponding PDE.</p>

<p>Again, the SDE is written bellow but with a boundary</p>

\[dX(t) = \mu(X(t), t) dt + \sigma(X(t),t)dB(t),\quad \textcolor{red}{X(s) = x}, \quad t &gt; s \geq 0\]

<p>One observation here is that the Markov property holds as</p>

\[\mathbb{E}[g(X(T)) \mid X(t)] = \mathbb{E}[g(X(T))| \mathcal{F}_t]\]

<p>The following result states the solution of a PDE with boundary is the same with the expectation of the boundary function. Formally, if $f(x, t)$ solve the backward equation,</p>

\[L_t f(x, t) + \frac{\partial f}{\partial t}(x, t) =0, \quad \textcolor{red}{f(x,T) = g(x)}\]

<p>then</p>

\[f(x,t) = \mathbb{E}[g(X(T)) \mid X(t)=x]\]

<p>This result is quite simple to figure out by using the martingale property of $f(X(t), t)$ as in the previous section. Here is the outline of the proof</p>

\[\begin{aligned}
\mathbb{E}[f(X(T), T)|\mathcal{F}_t] &amp;= f(X(t), t) \\
\mathbb{E}[g(X(T))|\mathcal{F}_t]&amp;= f(X(t), t)\\
\mathbb{E}[g(X(T))|X(t)]&amp;= f(X(t), t)\\
\mathbb{E}[g(X(T))|X(t)=x]&amp;= f(X(t)=x, t)
\end{aligned}\]

<p>This result can further extend the case</p>

\[L_t f(x, t) + \frac{\partial f}{\partial t}(x, t) = \textcolor{blue}{-\phi(x)}, \quad \textcolor{red}{f(x,T) = g(x)}\]

<p>then</p>

\[f(x,t) = \mathbb{E}\left[g(X(T)) + \int_0^T \phi(X(s))ds \mid X(t)=x \right]\]

<p><strong>Feynman-Kac formula</strong> Also, it can even generalize in the following case which is known as Feynman-Kac formula</p>

\[L_t f(x, t) + \frac{\partial f}{\partial t}(x, t) = \textcolor{blue}{r(x,t)f(x,t)}, \quad \textcolor{red}{f(x,T) = g(x)}\]

<p>then</p>

\[f(x,t) = \mathbb{E}\left[g(X(T))\textcolor{blue}{\exp\left(-\int_t^T r(X(s),s)ds\right)} \mid X(t)=x \right]\]

<p><em>Proof sketch</em>
From Ito’s lemma</p>

\[\begin{aligned}
df(X(t), t) &amp;= \textcolor{blue}{\left(L_t f(X(t), t) + \frac{\partial f}{\partial t}(X(t), t)\right)}dt + \textcolor{red}{\frac{\partial f}{\partial x}(X(t), t)\sigma(X(t), t) dB(t)} \\
df(X(t), t) &amp; = \textcolor{blue}{r(X(t), t) f(X(t), t)}dt + \textcolor{red}{dM(t)}
\end{aligned}\]

<p>Here we used the PDE and defined $M(t)$ in the red term.</p>

<p>This is a linear SDE of Langevin type (see <a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">this</a>) and its solution is</p>

\[\begin{aligned}
f(X(T), T) = &amp; f(X(t), t) \exp\left(\int_t^T r(X(u), u) du\right) \\
&amp;+ \exp\left(\int_t^T r(X(u), u) du\right) \int_t^T \exp\left(\int_t^s r(X(u), u) du\right) dM(s) \\
g(X(T)) &amp;= \dots
\end{aligned}\]

<p>The term associated with $M(t)$ has zero mean is a martingale with zero mean.</p>

<h2 id="time-homogeneous-diffusions">Time Homogeneous Diffusions</h2>
<p>Now, consider a case of time-independent coefficients SDEs</p>

\[dX(t) = \mu(X(t))dt + \sigma(X(t))dB(t)\]

<p>If there is a unique solution for this, the the transition probability is <em>shift-invariant</em> or $P(y, t, x, s) = P(y, t-s, x, 0)$. In other words, this transition probability depends on the time difference $t-s$ regardless of the start time.</p>

<p><strong>Quick note</strong> The transition probability is defined as $P(y, t, x, s) = P(X(t) \leq y \mid X(s) = x)$, the condition distribution of $X(t)$ given $X(s)=x$. The transition direction here is $(x, s) \to (y,t)$.</p>

<p><strong>Backward and forward equation</strong> We will see a lot that there is a correspondence between SDE and PDE. To indicate the direction, we have <em>forward</em> PDEs modeling the state-time $(y, t)$ given $(x, s)$, while the solution of <em>backward</em> PDEs is a function of $(x, s)$ given $(y, t)$.</p>

<p>Based on the shift-invariance, the transition probability now is defined given only three variables $(t, x, y)$ as $P(t, x, y) = P(y, t + s, x, s) = P(y, t, x, 0)$. The backward equation for the time homogeneous diffusion is</p>

\[\frac{\partial p}{\partial t} = \frac{1}{2} \sigma^2(x)\frac{\partial^2 p}{\partial x^2} + \mu(x)\frac{\partial p}{\partial x}\]

<p><strong>Example</strong> Black-Scholes PDE</p>

\[dX(t) = \mu X(t) dt + \sigma X(t) dB(t)\]

<p>Use backward equation, the transtion probability density satisfies</p>

\[\frac{\partial p}{\partial t} = \frac{1}{2} \sigma^2 x^2\frac{\partial^2 p}{\partial x^2} + \mu x\frac{\partial p}{\partial x}\]

<p>The known result that the transition probability for this case is $P(y, t, x, s) = \Phi\left(\frac{\ln(y/x) - (\mu - \sigma^2/2)(t-s)}{\sigma \sqrt{t-s}}\right)$. Then the density is simply obtained by taking derivative and is the fundamental solution for the above PDE</p>

\[p(t, x, y) = \frac{\partial }{\partial y} \Phi\left(\frac{\ln(y/x) - (\mu - \sigma^2/2)(t-s)}{\sigma \sqrt{t-s}}\right).\]

<h2 id="remark">Remark</h2>

<p>The note here does not relate to any part of machine learning research but just is a fundamental background.</p>

<p>The main takeaway message here is that in this problem setting, if we look for expectation, we may need to solve a PDE corresponding to the diffusion process. 
Martingale plays a very important role in the above derivation.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Anh  Tong.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



    

  </body>

  <d-bibliography src="/assets/bibliography/">
  </d-bibliography>

  

</html>
