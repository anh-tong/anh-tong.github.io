<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Anh  Tong


  | Stochastic Calculus in Machine Learning

</title>
<meta name="description" content="Anh Tong personal homepage
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2021/application_machine_learning/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Stochastic Calculus in Machine Learning",
      "description": "Exploring some aspect of stochastic calculus in optimization. The blog is based on the slides of Prof. Maxim Raginsky at UIUC.",
      "published": "September 15, 2021",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Anh</span>   Tong
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/experience/">
                Experience
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Stochastic Calculus in Machine Learning</h1>
        <p>Exploring some aspect of stochastic calculus in optimization. The blog is based on the slides of Prof. Maxim Raginsky at UIUC.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p>Now, let’s talk about what cases we can apply techniques of stochastic calculus to machine learning. Just like financial data, if a machine learning model deals with time series, considering stochastic calculus to handle randomness is a promising direction. Stochastic calculus can also come to improve and provide insights for machine learning methods.</p>

<p>This section will  focus on presenting the connection between stochastic optimization and stochastic calculus. The content of this section follows closely <a href="http://maxim.ece.illinois.edu/pubs/columbia.pdf">the slides</a> by Maxim Raginsky.</p>

<p>Stochastic optimization is one of the most well-known methods in machine learning. The method tries to miminizing a function by updating the function parameters in the inverse direction of its gradient. It does not have access to the full gradient but a noisy gradient which is usually computed from a small bactch of data. So because of this noisy gradient, stochastic calculus probably is a right tool.</p>

<!-- TODO: the slides seems focusing on gradient descent -->

<p>Formally, machine learning problems often end up with miminizing</p>

\[F(x) = \mathbb{E}_\xi[f(x, \xi)]\]

<p>where the randomness presented by \(\xi\) comes from randomized batch generations. The next procedure is to take a series of stochastic gradient steps to reach to a (local) minima. Although these steps are discrete, we can generalize with a continous version where the evolution of the parameters can follow the following dynamic:</p>

<h3 id="background-of-stochastic-calculus-for-this-section">Background of stochastic calculus for this section</h3>

\[\frac{\text{d}X_t}{\text{d}t} = G(X_t, t, \omega)\]

<p>where \(G\) stands for a function computing gradients and is injected with a random variable \(\omega\).</p>

<p>Studing the dynamic of gradient descent methods has been seen in some work including Neural Tangent Kernel<d-cite key="neural_tangent_kernel"> </d-cite>.</p>

<h4 id="diffusion-process">Diffusion process</h4>

<p>A diffusion process is defined as a Markov process if it satisfies</p>

<ul>
  <li>(i) \(p(\lVert X_{t+h} - X_t \rVert &gt; r \mid X_t = x) = o(h),\)</li>
  <li>(ii) \(\mathbb{E}[(X_{t+h} - X_t) \mathbb{1}_{\{\lVert X_{t+h} - X_t \rVert \leq r\}} \mid X_t = x] = hb(x) + o(h),\)</li>
  <li>(iii) \(\mathbb{E}[(X_{t+h} - X_t)(X_{t+h} - X_t)^\top \mathbb{1}_{\{\lVert X_{t+h} - X_t \rVert \leq r\}} \mid X_t = x] = hA(x) + o(h),\)
for some \(b(\cdot) \in \mathbb{R}^d, A(\cdot) = A^\top (\cdot) \in \mathbb{R}^{d\times d}_+\).</li>
</ul>

<p>The above properties can intuitively interpreted as that there exists a local drift with amount of \(hb(x) + o(h)\) and a covariance matrix as local diffusion matrix \(hA(x) + o(h)\).</p>

<h4 id="kolmogrov-equation-constructing-diffusion-process">Kolmogrov equation: Constructing diffusion process</h4>

<p><strong>Theorem (Kolmogorov equation)</strong> Let \(b, A\) satisfy a “certain condition”, then there is a diffusion process \((X_t)_{t\geq 0}\), such that, for any \(f \in C^2\),</p>

\[\mathbb{E}[f(X_t)] = \mathbb{E}[f(X_0)] + \int_0^t \mathbb{E}[\mathcal{L} f(X_s)] \text{d}s,\]

<p>where,</p>

\[\mathcal{L}f(x) = b(x)^\top \nabla f(x) + \frac{1}{2} \text{tr}[A(x) \nabla^2 f(x)]\text{d}s\]

<p><em>Example of Brownian motion</em></p>

<p>As we know that Brownian motion \(W_t\) has zero mean (\(b=0\)) and covariance \(A = \mathbf{I}\), we can have the expectation</p>

\[\mathbb{E}[f(W_t)] = f(0) + \frac{1}{2}\mathbb{E}[\text{tr}(\nabla^2 f(x))]\]

<p>We shall see next that the parameters going through optimization trajectory will follow a stochastic process (diffusion process, to be exact), we are interested in understanding the output of this process through a function \(f\) in order to compare to optimum value.</p>

<h3 id="tackle-local-optima">Tackle local optima</h3>

<p>One way to somehow escape local mimina is to add a random/jitter to the gradient descent step</p>

\[x_{t + \text{d}t} = x_t - \nabla F(x_t) \text{d}t \quad \Rightarrow \quad X_{t+\text{d}t} \sim \mathcal{N}(x_t -\nabla F(x_t)\text{d}t, \textcolor{red}{(2/\beta \cdot \text{d}t) \mathbf{I}_d})\]

<p>The red term can be further generalized with Brownian motion, leading to a noisy gradient descent:</p>

\[X_{t + \text{d}t} = X_t - \nabla F(X_t)\text{d}t + \textcolor{red}{\sqrt{2/\beta} (W_{t + \text{d}t} - W_t)}\]

<p>And now, we can recognize something familiar to the Black-Scholes models. The equation here is called the Langevin diffusion which already have a rich literature in physics, Bayesian inference, MCMC.</p>

<h3 id="key-result">Key result</h3>

<p>Now, let’s go over the main result presented in this paper<d-cite key="key_result"> </d-cite>.</p>

<p><strong>Theorem</strong> With a “reasonable assumption” (smoothness, dissiparity) that</p>

\[\lVert \nabla F(x) - \nabla F(x') \rVert \leq M \lVert x - x' \rVert \quad, \quad x^\top \nabla F(x) \geq m \lVert x \rVert^2 - b\]

<p>Then,</p>

\[\mathbb{E}[F(X_t)] - \inf_x F(x) \leq \sqrt{c_{\text{LS}} D(\nu_0 \lVert \pi)} \exp(-t/\beta c_{\text{LS}}) + \frac{d}{\beta} \log \frac{\beta + 1}{d}\]

<p>where,</p>

<ul>
  <li>(i) \(\nu_0 = \mathcal{L}(X_0)\) which is the intial distribution</li>
  <li>(ii) \(D(\cdot \lVert \cdot)\) is the Kullback-Leibler distribution</li>
  <li>(iii) \(c_{\text{LS}}\) is the log-Sobolev constant of \(\pi\)</li>
</ul>

<p><em>How to intepret this theorem?</em> 
Well, recall that the minimum value is \(\inf_x F(x)\). We need to compare the optimum value with the expectation of \(F(X_t)\) at time \(t\). The expectation is necessary since we are working with stochastic process \(X_t\). 
So the above result is saying that the different between the value of the function evaluated over the process \(X_t\) and the minimum is <em>bounded</em>.</p>

<p><em>What can we say about the bound here?</em> 
Looking at the right-hand side, we can see that the bound decreases <em>exponentially</em> as the time \(t\) goes. This is quite good, proving that optimization converges very fast. Also, the second term of the right-hand side, \(\frac{d}{\beta}\log \frac{\beta + 1}{d}\), suggests that we may select \(\beta\)  not too large as well as not too small.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Anh  Tong.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



    

  </body>

  <d-bibliography src="/assets/bibliography/2021-09-15-application-machine-learning.bib">
  </d-bibliography>

  

</html>
