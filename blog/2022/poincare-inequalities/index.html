<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Anh  Tong


  | Markov process and Poincare inequality

</title>
<meta name="description" content="Anh Tong personal homepage
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2022/poincare-inequalities/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Markov process and Poincare inequality",
      "description": "How to bound the variance of a Markov dynamical system?",
      "published": "February 14, 2022",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Anh</span>   Tong
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/experience/">
                Experience
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Markov process and Poincare inequality</h1>
        <p>How to bound the variance of a Markov dynamical system?</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p>It has been a while since the last post. This post will take a fresh new topic on inequalities of dynamical systems or Markov processes specifically. All the content is based on an excellent (I think) <a href="https://web.math.princeton.edu/~rvan/APC550.pdf"> lecture note</a> from <a href="https://web.math.princeton.edu/~rvan/">Ramon Van Handel</a>.</p>

<p>The main setup here is that we consider a high-dimensional data consisting of $X_1, X_2,…, X_n$ . We want to understand the behaviors of any function $f$ on these data when assuming $X_1,…, X_n$ are random variables. The behaviors here can be the variance of $f$ (how much the function fluctuates) or the suprema charateristics of $f$. This post will focus on the variance. 
One of classical results is that we can bound the variance of $f(X_1, …, X_n)$ by some form of individual gradients on each dimension $1, …, n$ (kind of hand waving for not explaining this here).</p>

<p>The index sets $1, …, n$ are discrete and finite. In this setting, we consider the infinite case ${X}_{t\in T}$, where $T$ usually is a continous time index. At the end of this post, we see how the variance of $f$ is bounded by gradients as in Poincare inequality.</p>

<h3 id="makov-process">Makov process</h3>
<p>The results will be established within Markov processes which are consider as <strong>memoryless</strong> stochastic process</p>

\[\mathbb{E}[f(X_{t+s}| \{X_r\}_{r\leq t})] = P_sf(X_t)\]

<p>The term <strong>memoryless</strong> is coined because the whole history $\left{ X_{r} \right}_{r&lt;t}$ is disregared and the expectation only is expressed by the most recent one $X_t$.</p>

<p>The stationarity is defined when given a probability measure $\mu$</p>

\[\mu(P_tf) = \mu(f)\]

<p>This means that no matter what time it is, the measure is invariant. There are some results on stationary measures as follows:</p>

<p><strong>Lemma 1</strong> The following hold given stationary measure $\mu$</p>
<ol>
  <li>Contraction (decreasing in norm): $\lvert\lvert P_tf \rvert\rvert_{L^p(\mu)} \leq \lvert \lvert f \rvert\rvert_{L^p(\mu)}$</li>
  <li>Linearity: $P_t(\alpha f + \beta g) = \alpha P_t f + \beta P_t g$</li>
  <li>Semigroup: $P_{t+s}f = P_tP_s f$</li>
  <li>Conservative: $P_t 1 = 1$ a.s.</li>
</ol>

<p>For the constraction property, we can easily prove by Jensen’s inequality and the tower property $\lvert\lvert P_tf\rvert \rvert_{L^p}^p = \mathbb{E}[\mathbb{E}[(P_tf(X_0))]^p\mid X_0] =\mathbb{E}[\mathbb{E}[(f(X_t))]^p\mid X_0] \leq  \mathbb{E}[\mathbb{E}[(f(X_t))^p]\mid X_0] = \lvert\lvert f\rvert \rvert_{L^p}^p$.</p>

<p>The contraction also leads to the derease of variance as $t$ increases.</p>

<p><strong>Generator</strong> For the case of discrete Markov processes, we often work with transition probability. However, in our case, such notion does not exist. Instead, we need to define an operator as $\mathcal{L}$ as a generator</p>

\[\mathcal{L}f :=\lim_{t \to 0} \frac{P_t f - f}{t}\]

<p>which quantify the rate of change of operator $P_t$. In particular, we have</p>

\[\frac{d}{dt} P_t f = \lim_{\delta \to 0} \frac{P_{t + \delta}f - P_tf}{\delta} = \lim_{\delta \to 0} P_f(\frac{P_\delta f - f}{\delta}) = \lim_{\delta \to 0}\frac{P\delta P_t f - P_tf}{\delta} = P_t\mathcal{L}f = \mathcal{L}P_tf\]

<p>This provides the commutative property between $P_t$ and $\mathcal{L}$.</p>

<p><strong>Reversibility</strong> This is a rebranding term of self-adjoint where $\langle f, P_t g \rangle = \langle P_tf, g\rangle$. However, the implication here is that there is a backward process having the same law:</p>

\[\langle \textcolor{red}{P_t f}, g \rangle = \langle f, P_t g \rangle = \mathbb{E}[ f(X_0) \mathbb{E}[g(X_t)|X_0]] = \mathbb{E}[f(X_0) g(X_t)] = \mathbb{E}[\textcolor{red}{\mathbb{E}[f(X_0)|X_t]} g(X_t)]\]

<table>
  <tbody>
    <tr>
      <td>By the defition, the red terms lead to $P_t f(x) := \mathbb{E}[f(X_t)</td>
      <td>X_0=x] = \mathbb{E}[f(X_0)</td>
      <td>X_t=x]$. This is why we call it <em>reversibility</em>.</td>
    </tr>
  </tbody>
</table>

<p><strong>Note</strong> We can see many case studies, i.e., solution of stochastic differential equations (SDEs) satisfy the above definitions and properties. So it can be useful to analyze recent models in machine learning, e.g.,  Neural SDEs or score-based models, that attract a lot attention.</p>

<p><strong>Egordicity</strong> This notion usually is encountered when studying dynamical systems which converge to a stationary state no matter what initial values are. That is, $P_tf \to \mu f$ in $L^2(\mu)$ as $t\to 0$.</p>

<p>Before going to the main result, it is necessary to define one more concept which is Dirichlet form $\mathcal{E}(f, g)$</p>

\[\mathcal{E}(f, g) = - \langle f, \mathcal{L}g\rangle\]

<p><strong>Theorem</strong> Let $P_t$ be a reversible ergodic Markov semigroup with stationary measure $\mu$. The following are equivalent:</p>

<ol>
  <li>$Var_\mu[f] \leq c\mathcal{E}(f, f)$ for all $f$ (<span style="color:red">Poincare inequality</span>)</li>
  <li>$\lvert \lvert P_tf -\mu f \rvert \rvert_{L^2(\mu)} \leq e^{-t/c} \lvert \lvert f - \mu f \rvert \rvert_{L^2(\mu)}$</li>
  <li>$\mathcal{E}(P_tf, P_tf)\leq e^{-2t/c}\mathcal{E}(f, f)$</li>
  <li>$\lvert \lvert P_tf -\mu f \rvert \rvert_{L^2(\mu)} \leq \kappa(f)e^{-t/c}$</li>
  <li>$\mathcal{E}(P_tf, P_tf) \leq \kappa(f)e^{-t/c}$</li>
</ol>

<p><strong>Poincare inequality</strong></p>

<p>It is surprised that we can capture the dynamic of variance by a differential equation. This is the starting point of the proof.</p>

<p><strong>Lemma</strong> We have the identity of 
\(\frac{d}{dt}Var_{\mu}[P_tf]=-2\mathcal{E}(P_tf, P_tf)\)</p>

<p>Proof:</p>

\[\begin{aligned}
\frac{d}{dt}Var_\mu[P_tf] = &amp; \frac{d}{dt}\{\mu((P_tf)^2) - (\mu P_tf)^2\} &amp;&amp; \\
= &amp; \frac{d}{dt} \mu((P_tf)^2) &amp;&amp; ( \mu(P_tf) = \mu(f) \text{does not depend on }t)\\
= &amp; \mu(2P_tf \frac{d}{dt}P_tf) &amp;&amp;\\
= &amp; \mu(2P_tf\mathcal{L}P_tf) &amp;&amp; \\
= &amp; -2 \mathcal{E}(P_tf, P_tf) &amp;&amp; (\text{by definition of Dirichlet form})
\end{aligned}\]

<p>As the previous observation that $t \mapsto Var_\mu[P_tf]$ is a decrease function, then $\mathcal{E}(f, f) = -\frac{1}{2}\frac{d}{dt}Var_\mu[P_tf]\rvert_{t=0} \geq 0$.</p>

<p>Also by the egordicity $P_tf \to \mu f$ then $Var_\mu[P_tf] \to Var_\mu[\mu f] = 0$. Actually the variance can be written as the integration (solution) of above differential equation.</p>

\[Var_{\mu}[f] = \int_\infty^0 \frac{d}{dt}Var_{\mu}[P_tf]dt=2\int_0^\infty \mathcal{E}(P_tf, P_tf)\]

<p><strong><span style="color:red">(3 $\Rightarrow 1$): Assumming 3 satisfies, proving 1</span></strong> By 3, we have $\mathcal{E}(P_tf, P_tf)\leq e^{-2t/c}\mathcal{E}(f, f)$. Putting this into the integral</p>

\[Var_{\mu}[f] = 2 \mathcal{E}(f, f)\int_0^\infty e^{-2t/c}dt = c\mathcal{E}(f,f)\]

<p><strong><span style="color:red">(1 $\Rightarrow 2$): Assumming 2 satisfies, proving 1</span></strong> By assuming 1, we have $c \mathcal{E}(P_tf, P_tf) \geq Var_\mu[P_tf]$. Putting this into the variance identities</p>

<p>\(\frac{d}{dt}Var_\mu[P_tf]  \leq -\frac{2}{c} Var_\mu[P_tf]\)
As we all know that the solution of $\frac{d}{dt}x = a x$ is $x_t = x_0 e^{at}$. Then $Var_\mu[P_tf] \leq Var_\mu[P_0f]e^{-2t/c}$ which is further expanded as</p>

\[\lvert\lvert P_tf - \mu f \rvert \rvert_{L^2{\mu}}^2 =Var_\mu[P_tf] \leq Var_\mu[P_0f]e^{-2t/c} =  Var_\mu[f]e^{-2t/c} = e^{-2t/c} \lvert \lvert f-\mu f \rvert\rvert_{L^2(\mu)}^2\]

<p>It is not difficult to show (2 $\Rightarrow$ 1) by using the variance identity.</p>

<p>The rest of equivalence requires to take into account the egordicity that we have another identity.</p>

<p><strong>Lemma</strong> If the Markov semigroup $P_t$ is reversible, then $t \mapsto \log Var_\mu[P_t f]$ and $t \mapsto \log \mathcal{E}(P_tf, P_tf)$ are convex.</p>

<p>The main technique to prove the convexity here is to show the Hessian is nonnegative. Luckly, we can derive such Hessian information. The first-order derivative is</p>

\[\begin{aligned}
\frac{d}{dt}\mathcal{E}(P_tf, P_tf) &amp; = -\frac{d}{dt} \langle P_tf, \mathcal{L} P_tf\mathcal \rangle_\mu &amp;&amp; (\text{by definition of Dirichlet form}) \\
&amp; = -\langle \frac{d}{dt} P_tf,  \mathcal{L} P_tf\mathcal \rangle_\mu - \langle  P_tf,  \frac{d}{dt} \mathcal{L} P_tf\mathcal \rangle_\mu&amp;&amp; (\text{derivative of inner product}) \\
&amp; = -\langle \mathcal{L} P_tf,  \mathcal{L} P_tf\mathcal \rangle_\mu - \langle  P_tf,  \mathcal{L}^2 P_tf\mathcal \rangle_\mu&amp;&amp; (\text{by definition of generator}) \\
&amp; = -2 \lvert \lvert  \mathcal{L} P_tf \rvert \rvert_{L^2(\mu)}^2 &amp;&amp; (\mathcal{L} \text{ is self-adjoint})
\end{aligned}\]

<p>This will allow us to obtain</p>

\[\begin{aligned}
\frac{d^2}{dt^2}\log Var_\mu[P_tf] &amp;= \frac{d}{dt}\left(\frac{1}{Var_\mu[P_tf]}\frac{d}{dt}Var_\mu[P_tf]\right) &amp;&amp; \\
&amp; = \frac{d}{dt}\left(\frac{1}{Var_\mu[P_tf]} \times (-2) \mathcal{E}(P_tf, P_tf)\right) &amp;&amp; (\text{by the variance identity})\\
&amp; = \frac{4\lvert \lvert  \mathcal{L} P_tf \rvert \rvert_{L^2(\mu)}^2}{Var_\mu[P_tf]} - \frac{4\mathcal{E}(P_tf, P_tf)}{Var_\mu[P_tf]^2} &amp;&amp; (\text{derivative of product and the above result})\\
&amp; = \frac{4}{Var_\mu[P_tf]^2}\left\{Var_\mu[P_tf] \lvert \lvert  \mathcal{L} P_tf \rvert \rvert_{L^2(\mu)}^2 - \langle P_tf, \mathcal{L}P_tf \rangle_\mu^2\right\}
\end{aligned}\]

<p>The nonnegativity of the right hand side is derived from the fact that $\mathcal{L}$ is self-adjoint, $\mathcal{L}1 = 0$ and Cauchy-Schwarz inequality</p>

\[\begin{aligned}
\langle P_tf, \mathcal{L}P_t f \rangle_\mu^2 =&amp; (\langle P_tf, \mathcal{L}P_t f \rangle_\mu - \underbrace{\langle \mathcal{L} \mu f, P_tf \rangle}_{0})^2 &amp;&amp; \\
= &amp; (\langle P_tf, \mathcal{L}P_t f \rangle_\mu - {\langle  \mu f, \mathcal{L}P_tf \rangle})^2 &amp;&amp; (\mathcal{L} \text{ is self-adjoint})\\
= &amp; \langle P_tf - \mu f, \mathcal{L}P_t f \rangle_\mu^2 &amp;&amp; \\
\leq &amp; \lvert \lvert  P_tf - \mu f \rvert \rvert_{L^2(\mu)}^2\lvert \lvert  \mathcal{L} P_tf \rvert \rvert_{L^2(\mu)}^2 &amp;&amp; (\text{Cauchy-Schwarz inequality})\\
= &amp; Var_\mu[P_tf] \lvert \lvert  \mathcal{L} P_tf \rvert \rvert_{L^2(\mu)}^2
\end{aligned}\]

<p>This makes the Hessian nonnegative. Therefore, we can conclude about the convexity of $t \mapsto \log Var_\mu[P_tf] $. For the case $t \mapsto \log \mathcal{E}(P_tf, P_tf)$, we can obtain the same conclusion but with the inequality $\mathcal{E}(f, g)^2 \leq \mathcal{E}(f, f)\mathcal{E}(g, g)$ from the fact that $\mathcal{E}(f + tg, f+tg) \geq 0$.</p>

<p><strong><span style="color:red">(2 $\Rightarrow 3$): Assumming 2 satisfies, proving 1</span></strong> As 2 is true. This means $\lvert \lvert P_tf -\mu f \rvert \rvert_{L^2(\mu)} \leq e^{-t/c} \lvert \lvert f - \mu f \rvert \rvert_{L^2(\mu)}$.</p>

<p>From the convexity of $t \to \log Var_\mu[P_tf]$, the first-order derivative function</p>

\[t \mapsto \frac{d}{dt} \log Var_\mu[P_tf] = {\frac{1}{Var_\mu[P_tf]} \frac{d}{dt}Var_\mu[P_tf] } = \frac{-2 \mathcal{E}(P_tf, P_tf)}{Var_\mu[P_tf]}\]

<p>is an increasing function. Therefore,</p>

\[\frac{-2 \mathcal{E}(P_tf, P_tf)}{Var_\mu[P_tf]} \geq \frac{-2 \mathcal{E}(P_0f, P_0f)}{Var_\mu[P_0f]} = \frac{-2 \mathcal{E}(f, f)}{Var_\mu[f]}\]

<p>This leads to</p>

\[\frac{\mathcal{E}(P_tf, P_tf)}{\mathcal{E}(f, f)} \leq \frac{Var_\mu[P_tf]}{Var_\mu[f]} = \frac{\lvert \lvert P_tf -\mu f \rvert \rvert_{L^2(\mu)}^2}{\lvert \lvert f - \mu f \rvert \rvert_{L^2(\mu)}^2} \leq e^{-2t/c}\]

<p>The remaining proof for other implication is derived similarly.</p>

<h3 id="conclusion">Conclusion</h3>
<p>This post studies the inequality involves the variance of dynamically systems. As the beginning of the post, the variance will be bounded by gradient but in fact, we find the bound in Dirichlet form. However, this form will be coresponding to gradient information. For example, in the particular case (Ornistein-Uhlenbeck process) of $P_tf(x) = \mathbb{E}[f(e^{-t}x + \sqrt{1 - e^{-2t}}\xi)], \xi\sim \mathcal{N}(0,1)$, the Dirichlet form is $\mathcal{E}(f, g) = \langle f^\prime, g^\prime\rangle_\mu.$</p>

<p>I anticipate that such inequalities can be useful for understanding any quantities, e.g. $f$ as a likelihood function, related to  Neural SDE, or score-based generative models. The variance identities somehow resembles the continous normalizing flow models where the log-density funcion is also governed by a differential equation.</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Anh  Tong.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



    

  </body>

  <d-bibliography src="/assets/bibliography/">
  </d-bibliography>

  

</html>
